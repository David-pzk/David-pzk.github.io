<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>David</title>
  
  <subtitle>Love the sunshine ,Enjoy the rainy day!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://david-pzk.github.io/"/>
  <updated>2018-12-18T08:46:13.181Z</updated>
  <id>https://david-pzk.github.io/</id>
  
  <author>
    <name>David-pzk</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>自然语言处理之情感分析模型——ACAEC</title>
    <link href="https://david-pzk.github.io/2018/12/13/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94ACAEC/"/>
    <id>https://david-pzk.github.io/2018/12/13/自然语言处理之情感分类模型——ACAEC/</id>
    <published>2018-12-12T16:00:00.000Z</published>
    <updated>2018-12-18T08:46:13.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自然语言处理之情感分析模型——ACAEC"><a href="#自然语言处理之情感分析模型——ACAEC" class="headerlink" title="自然语言处理之情感分析模型——ACAEC"></a>自然语言处理之情感分析模型——ACAEC</h1><p>“除了眼前璀璨的星空，不要忘记我们脚下的阶石，那是我们习以为常但前人穷其一生的。”</p><div align="right">——小记</div><p>&#160; &#160; &#160; &#160;这是我在情感分析模型中读过的第二篇论文，从这篇论文中我看到了一个与第一篇论文不一样的情感分析模型，作者Murtadha Talib AL-Sharuee、刘飞等人使用了一个无需人工标注，并且可夸多个领域的基于无监督算法的情感分析模型——自动上下文分析和集合聚类方法的情感分析模型(ACAEC)。  </p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>&#160; &#160; &#160; &#160;情感分析分为基于词典和学习系统两个部分。通过词典来对短语的得分进行判断，以对文本进行有效的标记，这个过程往往也分成人工和自动生成两个方式。当然，人工标注可以提高标注的准确性，但同时意味着大量的耗费。本文中作者使用了自动生成的词典。其次作者针对监督学习的算法的领域依赖性进行分析，并且监督学习使用的训练集又都是领域相关的，所以带有更多的领域相关性。作者针对高人工解如和领域依赖问题，提出了使用无监督算法来姐姐情感信息的分类。<br>&#160; &#160; &#160; &#160;李和刘曾在论文中提出了使用k-means算法与投票机制相结合的方式进行的情感聚类模型，他们使用投票机制来解决k-means的不稳定性和决定文档的组类关系。同时使用TFIDF权重系统来进行形容词和副词的标记，提高了15%的实验精度。除此之外，他们还使用了kamps等人的词网络获得短语分数。但是这种基于随机质心选择的表现十分不稳定，而且每次进行新数据的处理时会重新对种子进行选择。其他作者也在对k-means进行分析，结果各有春秋。<br>&#160; &#160; &#160; &#160;除了聚类算法的选择， 作者还在预处理阶段进行了实验，其中包括上线问分析和非随机初始点的选择。</p><h2 id="自动语境分析和集成分类-ACAEC-："><a href="#自动语境分析和集成分类-ACAEC-：" class="headerlink" title="自动语境分析和集成分类(ACAEC)："></a>自动语境分析和集成分类(ACAEC)：</h2><p>&#160; &#160; &#160; &#160;该系统的设计初衷是为了解决领域依赖和标记成本的问题。ACAEC是一个双阶段混合算犯法，第一阶段时数据的准备和语境的分析，该步骤时使用特殊词典处理一般的语言现象（如：增强词、否定词、对比词）来实习那准备和清洗文本数据。第二阶段则是用无监督聚类算法集成k-means为基础实现几个数据集的展示。<br>&#160; &#160; &#160; &#160;在上下文分析中，基于情感词典极性分数构建词典可用有效切勿领域精度的词典。增强词和否定词处理仅仅试下利用词典进行添加和替代情感表达短语时使用，而不是调整文献中常见的单次分数。通过有效捕获所表达的情绪来增强聚类的效果。<br>&#160; &#160; &#160; &#160;在第二阶段，作者提供了一个非随机初始化，提高了系统在精确度和稳定性上的总体镖行啊。在AEAEC中，作者提供了一个不服在并且计算要求不高的初始化途径。初始化新城区利用情感词网络构建的特征空间中，聚类的阶石问题也在使用极性质心的过程中被巧妙解决。<br> &#160; &#160; &#160; &#160;系统的表现和可靠性也在通过使用在有效性方面进行实验测试的权重方案混合不同的数据向量中得到提升。</p><h3 id="情感词网络"><a href="#情感词网络" class="headerlink" title="情感词网络"></a>情感词网络</h3><p> &#160; &#160; &#160; &#160;在该过程中使用了特殊化的情感词网络。情感词网络是一个具有三种分数（积极、消极和客观）的自动生成词典。我们会对每一个短语的极性用分数评估，通过该术语在每一个特类别中的分数，这些分数的和为1，并且每个类别都有一个基于调用情绪强度的部分值。由八个单元分类器组成的协议将会别用于构建这个词典。在系统中作者使用了基于wordnet3.0的sentiwordnet3.0来进行构建，除了分类协议外还是用了随即不来增强分数，实际提高了19%的分类准确度。<br> <img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/1.png" alt=""><br> 相同的情感词可能具有不同的分数，因为他可能出现不同的同义词集中。使用一个独立的同义词集的值需要文本模糊分析方法。所以，同义词集分数的平均值将会被用在构建辞典U。获得的辞典包含形容词和副词与他们的平均分数。这将会被用在语境分析和上下文分类的阶段中。  </p><h3 id="自动语境分析："><a href="#自动语境分析：" class="headerlink" title="自动语境分析："></a>自动语境分析：</h3><p> &#160; &#160; &#160; &#160;上下文分时时ACAEC的第一部分，其中包含五个自动且连贯的过程用来准备评论集和解决普遍的语义结构。SentiWordNet可以被用来有效生成这些过程中的特定辞典。五个步骤为：数据准备，拼写矫正、情绪控制、否定词控制和对比词控制。    </p><h4 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h4><p>  &#160; &#160; &#160; &#160;该步骤在处理网页文本时是必要的。清洗系统是基于角色的，并且要清除重复评论和标记符号，同时可以建文本转换成小写，并且分离未分离的句子和标记，这可以事接下来的工作中句子边界的加检测和标记更加精确。</p><h4 id="拼写检查"><a href="#拼写检查" class="headerlink" title="拼写检查"></a>拼写检查</h4><p> &#160; &#160; &#160; &#160;拼写检查在系统种占有重要的地位，以为如果是一个拼写错误的单词将无法应用在之后的词性分析和辞典中。因此纠正尽可能多的错误拼写将有助于提高在之后的发挥，尤其是在大量的错误的形容词和副词中，因为他们将用来在词性分析中作为特征集。拼写错误的单词使用两个词典进行纠正，一个词典是一般词典，之后使用SentiWordNet2派生的专用词典来纠正评论中的形容词和副词。  </p><h4 id="增强处理"><a href="#增强处理" class="headerlink" title="增强处理"></a>增强处理</h4><p> &#160; &#160; &#160; &#160;一个增强词往往是一个量化形容词的副词。增强词是一个普通而又有效的情感增强装置，因此解决增强词可以改善在之后的算法效果。我们使用在情感词网络中生成的词典U来进行相关的处理，在该词典中，无论是形容词还是副词都是根据情感得分来判定情绪之间的相关信息。关注形容词和副词是因为同盟可以形成用于分类的特征集。定义增强器的预定义列表并在识别增强器的过程中使用。ACAEC通过将一个同意短语替换增强词。<br> &#160; &#160; &#160; &#160;系统通过将增强词替换为有相同意义的形容词，通过这种方式，可以通过添加将被提取作为算法的下一学习阶段的特征的同义词来检测被调用的情绪，无论是正面还是负面。  </p><h4 id="否定处理"><a href="#否定处理" class="headerlink" title="否定处理"></a>否定处理</h4><p>  &#160; &#160; &#160; &#160;另一个普遍的准确语法形式是否定词，它将改变一个属于的极性至相反的方向。为了确定否定的表达形式，我们使用一个预定义的列表来表示否定词。为了处理这些否定词，偶尔没呢建立一个近义词词典来把那些在否定词后的形容词和副词替换为与他们相反的词。<br>  &#160; &#160; &#160; &#160;文献70使用了相似的概念来实现这样的步骤，只不过文献中只处理了积极和消极的形容词和副词，而本文实现了情感词网络构建词典。这个词典为之前在情感词网络中构建的词典U。反义词在情感强度方面是反义词，无论其实际含义如何。 当使用基于规则的方法来处理否定时，需要指定那些位于接近否定项并且可能被否定的单词的范围。 我们测试了不同的范围，并通过实验发现，否定术语后的五字范围是最有效的。</p><h4 id="对比处理"><a href="#对比处理" class="headerlink" title="对比处理"></a>对比处理</h4><p>  &#160; &#160; &#160; &#160;对比词是另一个普遍的英语语言结构。当一个句子包含一个对比句式，往往在这之后的是作者的真实观点。这将决定整个句子的情感。为了确定对比词，系统预定义了一个词典包含这些词语。将整个句子的词语与这些词进行对比，以找到对比词，将重要的句子记录，并将无关的句式排除。</p><h3 id="集成分类"><a href="#集成分类" class="headerlink" title="集成分类"></a>集成分类</h3><p>&#160; &#160; &#160; &#160;集成学习是一个有效的技术，尤其是当目标数据复杂并且可以用多种形式表示。尽管集成学习相对于单独的学习算法有更高的夫再度，但它可以生成高度多样性的模型从而提高精确度和泛化能力。该文章中使用这种具有多个特征向量模型的拘束，每个模型以一种独立的权重代表数据。而集成学习的基本成分使用的K-means算法。</p><h4 id="k-means算法"><a href="#k-means算法" class="headerlink" title="k-means算法"></a>k-means算法</h4><p>&#160; &#160; &#160; &#160;k-means算法一个统计并且传统的算法，其具有强硬的边界，所形成的簇不具有共享实例。<br>&#160; &#160; &#160; &#160;这个算法适合该技术的原因：1、该算法为无监督算法，是个无领域依赖性的系统；2、k-means总是具有较少的迭代次数，底数量的迭代次数也是合适的第一质心选择的结果；3、 虽然预定义K-means的数量并且硬聚类可以被认为是k-means的缺点，但是这对于该系统足够了，因为ACAEC本身就只进行积极和消极两类分类，这样我们也可以使用提前定义聚类的数量。4、k-means不稳定性也通过非随机初始化的过程规避，并且这样可以提高它的精度。<br>&#160; &#160; &#160; &#160;k-means初始时随机选择质心，之后计算每个数据点相对于质心的距离，形成各个组类，可以按照余弦距离、欧式距离过着其他合适的度量系统。下一步则是计算各个组别的平均值，选取该值作为新的质心，形成新的组别。重复以上步骤，知道没有找到新的质心，则说明收敛。<br>&#160; &#160; &#160; &#160;k-means的影响有insulting有一下几点：1、初始的质心选择；2、数据特征；3、距离测试系统。系统选择余弦距离是因为之前的实验证明余弦距离具有更加精确的效果。之后，将展示将sentiwordnet和数据特征用来提高精度。  </p><h4 id="空间向量模型"><a href="#空间向量模型" class="headerlink" title="空间向量模型"></a>空间向量模型</h4><p>&#160; &#160; &#160; &#160;向量空间事文本分析常用的表示形式，短语为其特征，而文本则会被观察。在系统中文档往往被形容词和副词表示，因为这些词是词性分析中带有情感表达。在所提出的系统中，我们可以将24个向量空间在不同的数据集中进行实验，以获得更高精确度的结果。因为这是无监督系统，并且使用了k-means的极性非随机初始化聚集，这相对于分层的聚类算法更有效，可以使得计算复杂度降低。为了构建不同的向量空间模型，存在矩阵和频率矩阵被采用。<br>&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;存在矩阵：这通过二进制向量表示每个文档，其中值1.0表示文档中特定特征的存在。<br>&#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160;频率矩阵：用一组向量空间中的向量表示每个文档，每个值为f的对数，f为出现在文档中的特征次数。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/2.png" alt=""><br>对于两个矩阵，在实验中使用以下权重，结果显示在表3和4中。除了这些权重之外，通过将词典U（参考算法1）的分数添加到每个矩阵来增加VSM数（ 图3）。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/3.png" alt=""><br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/4.png" alt=""></p><h5 id="短语规范化"><a href="#短语规范化" class="headerlink" title="短语规范化"></a>短语规范化</h5><p>&#160; &#160; &#160; &#160;TN测量的是一个短语在特定文章中的重要性，分子表示的是一个词的数量，分母表示的是文档中的字数。该系统是为了在考虑文档字数的基础上考虑词的重要性。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/5.png" alt=""><br>ti为短语i的频率，lj为文档j的长度。</p><h5 id="逆文档频率（IDF）："><a href="#逆文档频率（IDF）：" class="headerlink" title="逆文档频率（IDF）："></a>逆文档频率（IDF）：</h5><p>&#160; &#160; &#160; &#160;随着短语重要性过频率测量，IDF通常被用来作为另一种权重系统。它用来测量该短语在语料库中的重要性，而不是在某个特定的文档中。通过实验中的观察，IDF在某些情况下比讲它于其他属于重要性测量方式结合更有效。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/6.png" alt=""><br>D表示所有文档的数量，而dfi表示有短语i出现的文档的数量。  </p><h5 id="Term-frequency-inverse-document-frequency-TFIDF"><a href="#Term-frequency-inverse-document-frequency-TFIDF" class="headerlink" title="Term frequency inverse document frequency(TFIDF)"></a>Term frequency inverse document frequency(TFIDF)</h5><p>&#160; &#160; &#160; &#160;TFIDF是一种似乎合理并且普遍使用在文本挖掘的计分程序中。它测量一个特定词的重要性不仅仅是在文本中，也同时考虑在语料库中你文档频率。TFIDF与术语频率值成比例并抵消逆文档频率值。 它通过方程式在数学上表示。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/7.png" alt=""><br>Tfi,j为短语i的常规化，idfj是短语i的逆文档频率。</p><h5 id="Weight-frequency-inverse-document-frequency-WFIDF"><a href="#Weight-frequency-inverse-document-frequency-WFIDF" class="headerlink" title="Weight frequency inverse document frequency(WFIDF)"></a>Weight frequency inverse document frequency(WFIDF)</h5><p>&#160; &#160; &#160; &#160;WFIDF是一个长用于改善文本挖掘系统精度的权重计算系统。这是对使用术语频率的缺点的提出的解决方案，其是假设文档中术语的出现次数等于单次出现的重要性的计数。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/8.png" alt=""><br>Tfi是短语i的标准化,idfi是短语i的逆文档频率。</p><h5 id="Average-of-weights-AW"><a href="#Average-of-weights-AW" class="headerlink" title="Average of weights(AW)"></a>Average of weights(AW)</h5><p>&#160; &#160; &#160; &#160;AW是TFIDF和WFTDF两个值的平均值，自然该值比单独使用两个值有更高的精度。下面公式表示短语i在文档j中的值。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/9.png" alt=""></p><h4 id="中立短语和特征简化"><a href="#中立短语和特征简化" class="headerlink" title="中立短语和特征简化"></a>中立短语和特征简化</h4><p>&#160; &#160; &#160; &#160;中立短语属于一种冗余特征，所以特征简化是必要的步骤。在移除中性特征之后，应该在使用其他特征选择方法之前仔细考虑，因为它可能导致高稀疏度向量空间中的短文档的不准确聚类。</p><h5 id="极性质心"><a href="#极性质心" class="headerlink" title="极性质心"></a>极性质心</h5><p>&#160; &#160; &#160; &#160;在我们的系统中，我们仅使用两类极性质心，积极和消极。两质心都是自动从特征集F中是选出来，之后将应用在之后的语料库D中，这将通过比较每个特征集中特征和字典U来实现。</p><h5 id="非随机极性初始化点"><a href="#非随机极性初始化点" class="headerlink" title="非随机极性初始化点"></a>非随机极性初始化点</h5><p>&#160; &#160; &#160; &#160;k-means算法的初始点是构成最终聚类的重要因素，所以处理聚类的过程和结果于初始点进行的第一次迭代秘密相关。因此处理质心的选择是迭代数量和算法覆盖程度的主要影响因素。<br>&#160; &#160; &#160; &#160;在一般的k-means算法中，常常使用随机初始化质心来进行聚类，但在二极分类中，这往往会导致不精确的分类结果，因为两个初始质心属于同一类。为了解决这个问题，有人提出了k-means++算法，该算法会选择距离较远的两个质心来进行聚类。然而，尽管选择了不同的质心，但是与任何其他文档不相关且不相似的初始非信息点或离群点的选择是聚类方法性能降低的另一个原因。<br>&#160; &#160; &#160; &#160;其他的研究建议使用遗传算法来解决这个问题。运行多次也是一种非解决问题的途径。在文献35中，几个k-means运行在一起的结果，通过投票算法来实现，但是特们的系统仍然基于一个随机的初始化并且这并不能完全取出不稳定的问题。<br>&#160; &#160; &#160; &#160;在ACAEC系统中，Spos和Sneg在算法3中实习，被用来作为k-means算法中的非随机初始化起点。这两个极性质心将被保证处于不同的类别中，并且总是有效，可以与大多数被处理的文本相关。这种初始化方式避免了不稳定的问题，因为k-means总是保证聚集的簇处于相同的类别。这是一个具有低计算量和简易途径可以解决k-means算法并改善聚类过程。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/10.png" alt=""></p><h5 id="集群解释"><a href="#集群解释" class="headerlink" title="集群解释"></a>集群解释</h5><p>&#160; &#160; &#160; &#160;K-means算法要求我们在对显示生活中的数据进行分类时能对类别解释，因为在该算法中并未提供任何标签，我们要做的就是标注他们是积极还是消极。我们使用算法3来决定分组的极性，我们可以推断积极的簇为使用积极质心聚集的，而消极的簇为使用消极质心聚集的。而质心能够被高度划分则是因为所有的极性特征被分布在两者之间。所以，将所有质心分到合适组类是k-means最直接的方式。在实验结果的官场中，微弱的集成成分也能轻松分配每个类别进入合适组别。<br>&#160; &#160; &#160; &#160;作者将该实验与li的实验进行比对，检查连实验将积极质心和消极质心错误分类的情况。li的系统建立在使用混淆矩阵中，但(b+c)&gt;=(a+d)时，表明该分类分类正确，反之亦然。通过多个实验，没有相对的解释出行啊在使用混淆矩阵和使用质心的系统中。<br>&#160; &#160; &#160; &#160;但是，当有两个质心出现在同一个组别中，也就是有一个被错误的分类，系统就无法对组别进行解释。所以集成成分的结果会被用在集成中。最后，利用质心可以被认为是一种有利组别解释，因为一个集成成分总是会正确分类或者建一个错误分类，但这是可以在集成系统中被忽略的。</p><h4 id="集成学习："><a href="#集成学习：" class="headerlink" title="集成学习："></a>集成学习：</h4><p>&#160; &#160; &#160; &#160;集成学习是一种将多个学习算法结合以获得更高的精确度。它可以结合相同类型的学习算法，比如：baging和boosting集成系统。它同时也能集成不同的算法。集成学习可以被用来在大多数监督算法中来实习那情感分析。他们的不同在于基础分类器的特征选择和学习与分类器的结合系统上。一种观点认为，一个集成系统能够比一个单独的分类器更加的精确。一个准确的分类器，也被称为弱分类器，是一种优于随机猜测的分类器。<br>&#160; &#160; &#160; &#160;一个集成学习系统能能够增强分类的效果，因为它的结果将收集基础学习结果并以一种合适的方式综合。最终，复杂的问题将会被解决，即使使用一种弱分类器进行混合。它还可以解决过度拟合问题，避免潜在的计算失败，例如局部最优的堆栈和解决使用单个学习者难以解决的复杂问题。种种优点驱使着我们检查一个集成系统的影响通过主流的投票将修改过的k-means算法的结果，预定义起始质心在不同的特征向量中。<br>&#160; &#160; &#160; &#160;不同的集成成分会通过不容的权重系统获得，同时特们的精度也会相比随机猜测初始值来的更加的精确。更重要的是，在ACAEC中，组装对于组类的识别非常重要。在集成学习中不准确的将极性指点错误分类的可能性是十分小的，因为大部分集成成分可以实现数据点的正确分类，而且这也可以被认为是组类定义的非常重要的确认。为了提高这样的精确度，当出现之前提到的弱分类器错误将数据分类到同一个类别中，我们可以忽略这样的结果。</p><h5 id="集成分类算法"><a href="#集成分类算法" class="headerlink" title="集成分类算法"></a>集成分类算法</h5><p><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/11.png" alt=""><br>&#160; &#160; &#160; &#160;将多种特征向量空间结合的想法并不只是使得集成学习更加可靠，它也能够提供更多的灵活性，因为一个由使用额外的权重系统或另一个成分算法应用的增强系统更适用于大量数据的分析中。当时，增加的集成途径可能会增加计算量，所以，另一个组成学习成分要被谨慎添加进算法中。  </p><h5 id="计算复杂度分析："><a href="#计算复杂度分析：" class="headerlink" title="计算复杂度分析："></a>计算复杂度分析：</h5><p>&#160; &#160; &#160; &#160;k-means算法的负载对为O(g(nkt))，n为数据集的数据含量，k为聚类器的数量，t为迭代的数量，最后ACAEC的复杂度为O(mg(nkt))。集成系统的复杂度与集成成分的数量m是线性相关的，并且也依赖于基本学习算法。另外，算法中使用的相似性度量使用的是余弦距离，它的计算量依赖于向量长度。所以，降维则是最直接改善其效果的方法。</p><h2 id="聚类算法的对比"><a href="#聚类算法的对比" class="headerlink" title="聚类算法的对比"></a>聚类算法的对比</h2><p>&#160; &#160; &#160; &#160;几种聚类算法将会在关于情感的多文档分类到k各类别中进行有效性的比较，通过比较也可以展示使用积极质心和消极质心在该方法上的优越性。<br>&#160; &#160; &#160; &#160;选出24个矩阵中的6个不同数据表示进行实验，结果在表5中展示，用现实的标签来显示聚类的解释，同时，为了评估，我们估计文档被误分类与总文档的比率。比较过程如下：<br><strong>k-means不同的初始方式:</strong><br>&#160; &#160; &#160; &#160;极性点：使用算法3中产生的积极和消极两个质心作为初始点训练k-means算法的重要步骤，也是集成算法的基础。<br>&#160; &#160; &#160; &#160;<em>k-means:</em> 简单的k-means算法从所给的向量空间中随机选取初始点。<br>&#160; &#160; &#160; &#160;<em>二次抽样k-means:</em> 预先对VSM的随机10％子样本进行聚类以选择初始起始质心。<br>&#160; &#160; &#160; &#160;<em>统一化k-means:</em> k个初始起始点是随机均匀绘制的，其中每个质心的值是从VSM中该值的最大和最小分量之间的间隔中选择的。<br>&#160; &#160; &#160; &#160;<em>k-means++：</em> 从初始化阶段开始，选择不同的初始质心，其中选择每个质心的概率与其总体潜在贡献成比例。<br>&#160; &#160; &#160; &#160;<em>围绕中心点的分割法(PAM):</em> 这解决了与k-means相关的k-medoids问题，当时这基于发现中心点而不是图心，在实验中，k-means ++用于找到第一个起始数据点，之后通过迭代计算距离和总成本来选择作为数据点的代表性中间点。通过最小化中心点的范围和分配每个带你到它最近的中西来形成聚簇。<br>&#160; &#160; &#160; &#160;<em>聚类大型应用(Clara)：</em> 每次在大型的数据集中随机的选择一个数据集，然后又重复的应用于PAM。每次迭代中，所有的数据都围绕在数据点周围，同时当中心点不再变化时算法停止。<br>&#160; &#160; &#160; &#160;<em>高斯混合模型(GMN)：</em> k-mean++原则用来初始化期望最大化算法以使完全协方差矩阵适应数据。它通过组建数据点实现相关的算法来完成其中的相关信息。<br>&#160; &#160; &#160; &#160;<em>模糊C-MEANS聚类：</em> 这是一个软聚类算法，通过关系分数来测量每个数据点于多个聚类的关系程度。<br>&#160; &#160; &#160; &#160;<em>凝聚层次聚类(FCM)：</em> 这是一个完全连接算法，开始时将所有数据集放置于同一类中，然后，在每个阶段，合并具有最小完整连杆距离的两个组，直到形成两个簇。</p><h2 id="实验和分析"><a href="#实验和分析" class="headerlink" title="实验和分析"></a>实验和分析</h2><p>&#160; &#160; &#160; &#160;为了评估系统，实验在不同的评论集中进行。通常，为了评估系统的效果，极性机器学习算法时会将数据分类测试数据和评估数据。但是，在ACAEC中，作者使用所有的数据进行聚类实验，因为这是无监督算法。积极和消极的标签会附着在每个文本上，它们被用来建立模糊矩阵。积极和消极的急行电是我们能够确认产生聚集簇的类别。<br>&#160; &#160; &#160; &#160;由于我们关心积极和消极类型，于是通过平过户准确率来进行评估。下面的公式是基于混淆矩阵和种子定位的准确度评估，除此以外，我们还及性能精确度，召回率和F测量的计算。<br><img src="http://pin4m4ioe.bkt.clouddn.com/ACAEC/12.png" alt="">        </p><p>&#160; &#160; &#160; &#160;ACAEC通过Java8和NETBEANS集成工具来进行实验。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&#160; &#160; &#160; &#160;在该文章中，作者讨论了一个完整的应用于情感分析的自动无监督机器学习系统。该系统包含了自动上下文分析和无监督集成聚类。无监督学习和可靠性是将所提出的方法与文献中的其他工作区分开的特征。ACAEC的可靠性源于将上下文分析和集成学习系统结合。这是一个具有较优准确度的无监督途径，其后，这是一个领域无关的分析系统。ACAEC解决了数据注释的问题，避免了一个昂贵的过程。<br>&#160; &#160; &#160; &#160;作为特征工作，我们将考虑一个基于情感强度的多累问题。通过老驴跟过的上下文分析和利用其他权重系统或其噶机器学习途径能够获得一个增强器。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;自然语言处理之情感分析模型——ACAEC&quot;&gt;&lt;a href=&quot;#自然语言处理之情感分析模型——ACAEC&quot; class=&quot;headerlink&quot; title=&quot;自然语言处理之情感分析模型——ACAEC&quot;&gt;&lt;/a&gt;自然语言处理之情感分析模型——ACAEC&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="NLP" scheme="https://david-pzk.github.io/categories/NLP/"/>
    
    
      <category term="Hexo" scheme="https://david-pzk.github.io/tags/Hexo/"/>
    
      <category term="Markdown - machinelearning - NLP - sentiment analysis" scheme="https://david-pzk.github.io/tags/Markdown-machinelearning-NLP-sentiment-analysis/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理——情感分析模型</title>
    <link href="https://david-pzk.github.io/2018/11/24/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/"/>
    <id>https://david-pzk.github.io/2018/11/24/自然语言处理——情感分析模型/</id>
    <published>2018-11-23T16:00:00.000Z</published>
    <updated>2018-11-24T12:51:21.741Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自然语言处理之情感分析模型-——————SOSML模型"><a href="#自然语言处理之情感分析模型-——————SOSML模型" class="headerlink" title="自然语言处理之情感分析模型 ——————SOSML模型"></a>自然语言处理之情感分析模型 ——————SOSML模型</h1><p>“从这篇文章开始，便是记录我在自然语言处理情感分析模型的学习之路，以期望能实现一个完整的情感分析模型”                                         </p><div align="right"> ———前记</div><p>&#160; &#160; &#160; &#160;Asad Abdi等人在情感分析模型中实现了一种基于机器学习的多文档情感概括分析模型，该模型结合了情感知识、深度学习系统、统计和语义知识，简称为SOSML模型。    </p><a id="more"></a><h2 id="相关知识概括"><a href="#相关知识概括" class="headerlink" title="相关知识概括"></a>相关知识概括</h2><p>&#160; &#160; &#160; &#160;在介绍SOSML模型前，我将首先根据该文介绍一下情感分析的相关寄出知识点，在后面的文章中，我也不再对该部分进行过多的赘述。</p><h3 id="情感分析（Sentiment-analysis）"><a href="#情感分析（Sentiment-analysis）" class="headerlink" title="情感分析（Sentiment analysis）"></a>情感分析（Sentiment analysis）</h3><p>&#160; &#160; &#160; &#160;SA是自然语言处理(NLP)领域中一个热门研究方向，它可以通过研究人们的情感为企业和政府所作出的决策产生一些影响。情感分析可分为句式分析、文本分析和方面分析，其中句式和文本分析仅是分析句子或文本的积极与消极的极性。而方面分析则可以检测人们喜欢与不喜欢的对象和具体的喜欢程度。  </p><p>&#160; &#160; &#160; &#160;情感分析的方式可以分为3种途径：(1)、基于机器学习，(2)、基于词典，(3)、混合方式。而常用的机器方法为支持向量机(SVM)、朴素贝叶斯(NB)、随机梯度向量(SGD)等。  </p><h3 id="文本摘要-Summarization"><a href="#文本摘要-Summarization" class="headerlink" title="文本摘要(Summarization)"></a>文本摘要(Summarization)</h3><p>&#160; &#160; &#160; &#160;文本摘要是概括文本情感特种的重要内容，文本概括可分为一般摘要和面向查询的摘要，一般摘要主要是对文本文意的概括，而面向查询的摘要则是根据用户的查询所进行的。其中对文本文意的概括还可以分为抽取和抽象两种方式，抽取是从文本种找寻代表语句，而抽象则是使用新的词与句来对文本进行概括。文本分析一般分为三个步骤：(1)、解释——处理输入文本，挑选显著特征，(2)、转化——将上部所得结果转化为一个摘要表示，(3)、生成——将摘要表示生成合适得摘要。由于大量文本充斥着人们得主观意见，因此生成简明信息的文本摘要和总结大量评论者和评论的情感摘要开始结合。</p><h2 id="SOSML系统架构"><a href="#SOSML系统架构" class="headerlink" title="SOSML系统架构"></a>SOSML系统架构</h2><p>&#160; &#160; &#160; &#160;SOSML主要基于句式分析，大致分为四个步骤：(1)预处理（Pre-processing）——用基础的自然语言方法处理评论文本(2)特征抽取（Feature extraction）——抽取一组特征改善分类质量(3)分类（Classification）——对评论文本进行分类(4)摘要生成步骤（The summary generation）检查冗余信息，生成最终摘要。</p><h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><p>&#160; &#160; &#160; &#160;预处理是为了对数据集采用基础语义功能使得数据更适合于文本挖掘技术。该步骤在SOSML系统中主要有一下功能：句式划分(sentence splitting)、词干提取(stemming)、停用词删除(stop-word deletion)和词性标注(part-of-speech(pos) tagging)。</p><h4 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h4><p>&#160; &#160; &#160; &#160;特征是一种能够抓取数据模型的文本属性。特征概括主要是为了抽取一系列特征以改善总体的文本分类质量。系统中特征提取的步骤如下：  </p><p><strong><em>（一）情感知识</em></strong><br>&#160; &#160; &#160; &#160;<em>情感词典特征(Sentiment lexicon feature)</em>——情感词典是一组用于表达积极或消极情感的词语，在本模型中，我们使用HCLr来抽取以出现频率为其值得积极或消极词汇。</p><p>&#160; &#160; &#160; &#160;<em>否定特征(Negation features)</em>——否定词汇为那些影响句式极性得词汇，如：”I do not like this chair”，其中得not改变了句式的情感极性。系统中记录否定词的数量。</p><p>&#160; &#160; &#160; &#160;<em>句式类型(Sentence types)</em>——主观和客观句，条件和疑问句。</p><p>&#160; &#160; &#160; &#160;<em>标点特征(Punctuations feature)</em>——系统中记录“！“与”？“的数量。</p><p>&#160; &#160; &#160; &#160;<em>POS feature</em>——系统中还要考虑每个单词的词性，记各词性的数量。  </p><p>&#160; &#160; &#160; &#160;<em>情感分数特征(Sentiment score feature)</em>——通过基于情感词典获得的分数也作为参考的分数。  </p><p><strong><em>（二）统计和语言知识</em></strong><br>&#160; &#160; &#160; &#160;通过统计和语言知识准备一下特征：句子所在位置、标题词、关键词、线索词和句式将的相似度。  </p><p><strong><em>（三）词嵌入模型</em></strong><br>&#160; &#160; &#160; &#160;为了将自然语言处理功能转换为机器学习算法，文本必须首先被转换为合适的向量。在该系统中使用word2vec技术来进行特征的转换。Word2vec是一种基于机器学习的词向量提取技术。它尝试了解词的含义以及词间的语义关系，也就是通过映射词向量进入另一个向量空间，语义相似的词将会有相似的代表向量并且这些词向量具有相似的意思。Word2vec基于Skip-gram和continuous bag-of-words(CBOW) 词向量计算模型。Skip-gram通过一个单词来预测全文，CBOW则是通过上下文来预测单词。<br>&#160; &#160; &#160; &#160;取词向量的过程主要是基于Skip-gram模型，通过对神经网络的不断优化来取出输入层到隐藏层的转换向量，该向量则为词向量，值得注意的是该过程的目的并不是将训练好的模型用来预测词向量概率，而是为了取出隐藏层的转换向量，该向量则为对应的词向量，具体内容我将会再后面的章节用代码实现，读者可参考该链接：<a href="https://blog.csdn.net/rlnlo2pnefx9c/article/details/78747970" target="_blank" rel="noopener">https://blog.csdn.net/rlnlo2pnefx9c/article/details/78747970</a>。<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/1.jpg" alt=""><br>该公式为Skip-learn的概率显示公式，旨在最大化平均对数向量。</p><p>&#160; &#160; &#160; &#160;SOSML系统主首先将句式中的单词通过word2vec转换为适当的词向量，然后将不同的单词串联起来形成句向量，同时结合（一）（二）点得到的各特征，组成混合向量以适用于分类算法。  </p><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>&#160; &#160; &#160; &#160;特征选择是为了选择一组对分类器重要的特征集，也就是移除对于分类器而言多余的特征集。该过程可以增加分类的准确度和改善分类器的运行时间或者减少特征空间的大小和该少分类的质量。</p><p>&#160; &#160; &#160; &#160;SOSML通过应用统计技术（如：information gain,Gain Ration,Relief-F,Symmetrical Uncertainty）来筛选出关键的特征。</p><p>&#160; &#160; &#160; &#160;<strong><em>Relief-F</em></strong>——该算法随机选择一个样本Xi,然后随机选择两个与之最近的邻近样本，一个与Xi属于同一类，另一个与之属于不同类。可参考<a href="https://blog.csdn.net/littlely_ll/article/details/71614826" target="_blank" rel="noopener">https://blog.csdn.net/littlely_ll/article/details/71614826</a><br>具体算法如下图所示：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/2.png" alt=""><br>&#160; &#160; &#160; &#160;<strong><em>Information gain(IG)</em></strong>——信息增益是用来选择决策树中分类特征。主要目标有：(1)选择有大量值得特征(2)决定特征得排序(3)决定有益于分类方法得特征。<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/3.png" alt=""><br>详细可参考：<a href="https://blog.csdn.net/qq_40587575/article/details/80219080" target="_blank" rel="noopener">https://blog.csdn.net/qq_40587575/article/details/80219080</a>  </p><p>&#160; &#160; &#160; &#160;<strong><em>Gain Ratio(GR)</em></strong>——GR为一种基于IG的特征选择算法，该值有标准化后的某特征的IG值评估得到。较高的GR值表明该特征适用于分类算法：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/4.png" alt=""></p><p>&#160; &#160; &#160; &#160;<strong><em>Symmetrical Uncertainty（SU）</em></strong>——SU算法使用称为对称不确定性的信息理论度量，因此SU（x，y）与SU（y，x）的SU（x，y）相同，因此它减少了所需的比较次数，其中x和y是两个特征。 设IG（x | y）为特征x的信息增益，H（x）为特征x的熵，H（y）为特征y的熵。使用以下等式计算SU：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/5.png" alt=""></p><p>&#160; &#160; &#160; &#160;经过特征抽取和特征选择，我们已经准备好适用于分类的文本或句式向量。</p><h3 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h3><p>&#160; &#160; &#160; &#160;SOSML系统中主要应用传统的机器学习算法，传统的机器学习算法一般为监督算法，将训练数据用于对模型的训练，再将训练模型用于未知的数据分类。</p><p>&#160; &#160; &#160; &#160;<strong><em>Support Vector Machine(SVM)</em></strong>——支持向量机为目前较为火爆的机器学习算法，该算法用于找到分割数据的“最大分类超平面”，<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/6.png" alt=""></p><p>w为超平面的法向量，b为超平面的偏置向量。通过将一式最小化即可找到划分数据集的最优分割平面。</p><p>&#160; &#160; &#160; &#160;<strong><em>Decision Tree(DT)</em></strong>——决策树算法使用属性结构对数据进行分类，决策树从单个节点开始，每个节点分支为可能的结果，叶节点为类标签，分支表示特征的连接，非叶子节点则为特征的分类条件。<br>&#160; &#160; &#160; &#160;<em>Navi Bays(NB)</em>——贝叶斯算法是基于概率的算法，估计每一个数值属于某个类别的准确概率。贝叶斯假设每一个特征都是互相独立的，一个样本c属于某个类别x的概率可由一下公式计算得到：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/7.png" alt=""></p><p>&#160; &#160; &#160; &#160;<strong><em>Logistic Regression (LR)</em></strong>——洛吉斯蒂克回归以一种基于一个特征集的统计方法得到数据的类别。它旨在找到一种模式用来描述标签结果和特征之间的关系：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/8.png" alt=""></p><p>&#160; &#160; &#160; &#160;在该系统中，作者使用了多种机器学习算法来进行数据的分类处理，其他还包括Random Frost(RF),K-Nearest Neighbour(KNN),Artifical Neural Network(ANN)算法，和之前的介绍的算法相同，这些算法都属于基础算法，在此不再详述，读者可自行百度。  </p><h3 id="情感得分"><a href="#情感得分" class="headerlink" title="情感得分"></a>情感得分</h3><p>&#160; &#160; &#160; &#160;在数据预处理步骤中，我们需要应用high-coverage lexical resourde(HCLr)来实现对单词情感得分的计算，由于每个词典都有其局限性，该系统集合了多个词典，提高了词典的覆盖程度：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/9.png" alt=""></p><p>&#160; &#160; &#160; &#160;在该系统中作者应用了Semantic Sentiment Approach(SSA)来实现对单词情感得分的计算：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/10.png" alt=""><br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/11.png" alt=""></p><p>&#160; &#160; &#160; &#160;系统还考虑了单词的上下文影响，也就是一个单词的极性可能会根据前后语句发生极性的逆转，如：The bed is not well,well 的极性就因为not的存在发生了逆转。常用的否定词如下：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/12.png" alt=""></p><p>&#160; &#160; &#160; &#160;系统还在句式分析上考虑罢了主客观语句、条件和疑问句、转折句式等。<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/13.png" alt=""></p><h3 id="统计和语言知识"><a href="#统计和语言知识" class="headerlink" title="统计和语言知识"></a>统计和语言知识</h3><p>&#160; &#160; &#160; &#160;文本总结的目标在于选择文档中最有意义的句子，所以确定这些特征有助于确认这个句子和改善总结的质量。在SOSML系统中，综合考虑了关键词、标题、句式位置、暗示词和句式相似性。<br>&#160; &#160; &#160; &#160;<strong><em>Key-word method</em></strong>——一个句子含有更多的关键词，可以说明这个句子有更高的代表性，采用如下公式计算：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/14.png" alt=""></p><p>TFi为关键词的频率，ND为文档的总数量，DFi为包含该关键词的文档数量。在SOSML系统中如果一个聚在含有一个关键词，则K(Si)=1;否则K(si)=0;  </p><p>&#160; &#160; &#160; &#160;<strong><em>Sentence Position methond</em></strong>——在一段文字中，有代表性的句子往往出现在段落的开头和段落的结尾，我们使用如下的公式来计算句子所在位置的分数：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/15.png" alt=""></p><p>N为句子的总数量，1&lt;i&lt;N  </p><p>&#160; &#160; &#160; &#160;<strong><em>Title method</em></strong>——出现在标题中的短语往往包含文本准确的含义，在SOSML系统中，如果一个句子含有一个标题词，则T(S)=1,否则T(S)=0。  </p><p>&#160; &#160; &#160; &#160;<strong><em>Cue method</em></strong>——线索短语往往能暗示后面句子的含义，所以系统中含有线索词的句子C(S )=1,否则C(S)=0。  </p><p>&#160; &#160; &#160; &#160;<strong><em>Sentence-to-sentences similarity based on the Vector Space Model(VSM):SSVSM</em></strong><br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/16.png" alt=""></p><p>&#160; &#160; &#160; &#160;<strong><em>Content Word Expansion(CWE)</em></strong>——是一种基于情感词相似性的方法，两个单词的语义相似度由以下步骤组成：    </p><p>&#160; &#160; &#160; &#160;<strong><em>Dice measure</em></strong>:<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/17.png" alt=""></p><p>Syns(w)是单词w基于wordnet的单词集。<br>&#160; &#160; &#160; &#160;<strong><em>Semantic Similarity Measurement(SSM)</em></strong>——该方法用来衡量两个语句的相似度，用一下的方式来表达：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/17.png" alt=""></p><p>&#160; &#160; &#160; &#160;<strong><em>Graph-based</em></strong>——在SOSML系统中，最后使用了每个句子再全文中的相似度来表现该句：<br><img src="http://pin4m4ioe.bkt.clouddn.com/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B/19.png" alt=""></p><p>T表示所有句子，sim(si,d)表示选定句Si与其他所有句子的相似度 。  </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&#160; &#160; &#160; &#160;高质量的文本摘要不仅必须是信息性的，还应该是简洁的（非冗余的）。必须删除冗余以避免重复信息。为此，该方法执行以下任务。设B 1 =∅表示空集，B 2 = {S i | i = 1,2 ……，N}表示每个句子的总相似度得分。 B 2的句子按其得分按降序排列。在第一步中，顶部句子从B 2移动到B 1。在第二步中，从B 2中选择下一个顶部句子，然后，在将当前句子添加到B 1之前，将检查该句子以确保它与上面的B 1的每个句子没有任何相似性度量。相似性阈值（ST）。如果是，则将句子添加到B 1中。否则，该句子将被删除。将重复这些步骤，直到满足最终摘要中的句子数。最后，集合B 1被视为最终摘要。为了估计ST的值，我们使用梯度搜索策略。我们实施了一组不同ST的实验，范围从0.1到0.9，以观察性能的变化。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;自然语言处理之情感分析模型-——————SOSML模型&quot;&gt;&lt;a href=&quot;#自然语言处理之情感分析模型-——————SOSML模型&quot; class=&quot;headerlink&quot; title=&quot;自然语言处理之情感分析模型 ——————SOSML模型&quot;&gt;&lt;/a&gt;自然语言处理之情感分析模型 ——————SOSML模型&lt;/h1&gt;&lt;p&gt;“从这篇文章开始，便是记录我在自然语言处理情感分析模型的学习之路，以期望能实现一个完整的情感分析模型”                                         &lt;/p&gt;
&lt;div align=&quot;right&quot;&gt; ———前记&lt;/div&gt;

&lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;Asad Abdi等人在情感分析模型中实现了一种基于机器学习的多文档情感概括分析模型，该模型结合了情感知识、深度学习系统、统计和语义知识，简称为SOSML模型。    &lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://david-pzk.github.io/categories/NLP/"/>
    
    
      <category term="Hexo" scheme="https://david-pzk.github.io/tags/Hexo/"/>
    
      <category term="Markdown" scheme="https://david-pzk.github.io/tags/Markdown/"/>
    
      <category term="machinelearning" scheme="https://david-pzk.github.io/tags/machinelearning/"/>
    
      <category term="NLP" scheme="https://david-pzk.github.io/tags/NLP/"/>
    
      <category term="Sentiment Analysis" scheme="https://david-pzk.github.io/tags/Sentiment-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://david-pzk.github.io/2018/10/14/hello-world/"/>
    <id>https://david-pzk.github.io/2018/10/14/hello-world/</id>
    <published>2018-10-14T02:25:31.300Z</published>
    <updated>2018-10-15T02:44:01.654Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hexo</title>
    <link href="https://david-pzk.github.io/2018/10/05/markdown/"/>
    <id>https://david-pzk.github.io/2018/10/05/markdown/</id>
    <published>2018-10-04T16:00:00.000Z</published>
    <updated>2018-11-16T05:45:06.365Z</updated>
    
    <content type="html"><![CDATA[<p>这是我所写的第一的hexo博客，这是一篇测试博客，我希望的我的机器学习之路可以再次上升到一个新的台阶。<br><a id="more"></a><br>本期我将熟悉logistic回归算法、随机梯度下降、随机森林以及k-means算法的相关实现，主要将以实验的方式实现其中的相关内容。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是我所写的第一的hexo博客，这是一篇测试博客，我希望的我的机器学习之路可以再次上升到一个新的台阶。&lt;br&gt;
    
    </summary>
    
      <category term="Markdown" scheme="https://david-pzk.github.io/categories/Markdown/"/>
    
    
      <category term="Hexo" scheme="https://david-pzk.github.io/tags/Hexo/"/>
    
      <category term="Markdown" scheme="https://david-pzk.github.io/tags/Markdown/"/>
    
  </entry>
  
</feed>
